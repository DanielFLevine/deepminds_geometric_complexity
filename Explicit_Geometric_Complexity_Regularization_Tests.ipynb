{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU5EDONQblb7"
      },
      "outputs": [],
      "source": [
        "!pip3 install functorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEjXpZzgyl77"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "from functorch import jacrev\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import autograd\n",
        "from torch.nn.functional import cross_entropy\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32PyTEakyx_v"
      },
      "outputs": [],
      "source": [
        "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0, 1)])\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=trans)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=trans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFfjgiPPy0LR"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "t_batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1xAo3wKy2_a"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(\n",
        "    dataset=mnist_trainset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "testloader = DataLoader(\n",
        "    dataset=mnist_testset,\n",
        "    batch_size=t_batch_size,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_bVciSpy41K"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "for data_point in mnist_trainset:\n",
        "    x_train.append(torch.flatten(data_point[0][0]))\n",
        "    y_train.append(data_point[1])\n",
        "x_train = tuple(x_train)\n",
        "x_train = torch.stack(x_train)\n",
        "# x_train = torch.flatten(x_train, start_dim=1)\n",
        "x_train = x_train.to(device)\n",
        "y_train = torch.LongTensor(y_train).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHB4U1bey6yq"
      },
      "outputs": [],
      "source": [
        "x_test = []\n",
        "y_test = []\n",
        "for data_point in mnist_testset:\n",
        "    x_test.append(data_point[0][0])\n",
        "    y_test.append(data_point[1])\n",
        "x_test = torch.stack(x_test)\n",
        "x_test = torch.flatten(x_test, start_dim=1)\n",
        "y_test = torch.IntTensor(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YZmpET6y84O"
      },
      "outputs": [],
      "source": [
        "hidden_size = 500\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model_logits = nn.Sequential(\n",
        "            OrderedDict([\n",
        "                ('linear1', nn.Linear(28*28, hidden_size)),\n",
        "                ('relu1', nn.ReLU()),\n",
        "                ('linear2', nn.Linear(hidden_size, hidden_size)),\n",
        "                ('relu2', nn.ReLU()),\n",
        "                ('linear3', nn.Linear(hidden_size, hidden_size)),\n",
        "                ('relu3', nn.ReLU()),\n",
        "                ('linear4', nn.Linear(hidden_size, hidden_size)),\n",
        "                ('relu4', nn.ReLU()),\n",
        "                ('linear5', nn.Linear(hidden_size, hidden_size)),\n",
        "                ('relu5', nn.ReLU()),\n",
        "                ('linear6', nn.Linear(hidden_size, hidden_size)),\n",
        "                ('relu6', nn.ReLU()),\n",
        "                ('linear7', nn.Linear(hidden_size, hidden_size)),\n",
        "                ('relu7', nn.ReLU()),\n",
        "                ('linear8', nn.Linear(hidden_size, 10))\n",
        "            ])\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            input_size = module.in_features\n",
        "            scale = 1/math.sqrt(input_size)\n",
        "            torch.nn.init.trunc_normal_(module.weight, mean=0.0, std=scale, a=-2*scale, b=2*scale)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.model_logits(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQJzAIpny_-W",
        "outputId": "300bc5dc-6a4a-4419-c89a-6adf8b435682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1 finished.\n",
            "Training loss is: 2.3037174743652344\n",
            "Training accuracy is: 0.17813333333333334\n",
            "Validation loss is: 2.302281141281128\n",
            "Validation accuracy is: 0.179\n",
            "L2 norm is 140.52698469161987 \n",
            "\n",
            "Epoch 2 finished.\n",
            "Training loss is: 2.303323304748535\n",
            "Training accuracy is: 0.14108333333333334\n",
            "Validation loss is: 2.3018643856048584\n",
            "Validation accuracy is: 0.1444\n",
            "L2 norm is 140.52956652641296 \n",
            "\n",
            "Epoch 3 finished.\n",
            "Training loss is: 2.3028902155558266\n",
            "Training accuracy is: 0.13623333333333335\n",
            "Validation loss is: 2.3013722896575928\n",
            "Validation accuracy is: 0.1386\n",
            "L2 norm is 140.53464150428772 \n",
            "\n",
            "Epoch 4 finished.\n",
            "Training loss is: 2.302337832132975\n",
            "Training accuracy is: 0.16415\n",
            "Validation loss is: 2.300708770751953\n",
            "Validation accuracy is: 0.1664\n",
            "L2 norm is 140.54297375679016 \n",
            "\n",
            "Epoch 5 finished.\n",
            "Training loss is: 2.3015418340047202\n",
            "Training accuracy is: 0.23051666666666668\n",
            "Validation loss is: 2.299710273742676\n",
            "Validation accuracy is: 0.2343\n",
            "L2 norm is 140.55613565444946 \n",
            "\n",
            "Epoch 6 finished.\n",
            "Training loss is: 2.3002661849975587\n",
            "Training accuracy is: 0.29636666666666667\n",
            "Validation loss is: 2.2980058193206787\n",
            "Validation accuracy is: 0.3025\n",
            "L2 norm is 140.57730507850647 \n",
            "\n",
            "Epoch 7 finished.\n",
            "Training loss is: 2.297815846761068\n",
            "Training accuracy is: 0.31211666666666665\n",
            "Validation loss is: 2.294332265853882\n",
            "Validation accuracy is: 0.3124\n",
            "L2 norm is 140.61462450027466 \n",
            "\n",
            "Epoch 8 finished.\n",
            "Training loss is: 2.289237989807129\n",
            "Training accuracy is: 0.2058\n",
            "Validation loss is: 2.2696425914764404\n",
            "Validation accuracy is: 0.2069\n",
            "L2 norm is 140.71758699417114 \n",
            "\n",
            "Epoch 9 finished.\n",
            "Training loss is: 2.2228543431599936\n",
            "Training accuracy is: 0.3596666666666667\n",
            "Validation loss is: 2.15262508392334\n",
            "Validation accuracy is: 0.3635\n",
            "L2 norm is 141.0129108428955 \n",
            "\n",
            "Epoch 10 finished.\n",
            "Training loss is: 1.997040636698405\n",
            "Training accuracy is: 0.6227\n",
            "Validation loss is: 1.861185908317566\n",
            "Validation accuracy is: 0.6272\n",
            "L2 norm is 141.6363925933838 \n",
            "\n",
            "Epoch 11 finished.\n",
            "Training loss is: 1.797225200398763\n",
            "Training accuracy is: 0.7253\n",
            "Validation loss is: 1.7422152757644653\n",
            "Validation accuracy is: 0.7336\n",
            "L2 norm is 142.0353467464447 \n",
            "\n",
            "Epoch 12 finished.\n",
            "Training loss is: 1.737117800394694\n",
            "Training accuracy is: 0.74265\n",
            "Validation loss is: 1.7192031145095825\n",
            "Validation accuracy is: 0.7479\n",
            "L2 norm is 142.21736884117126 \n",
            "\n",
            "Epoch 13 finished.\n",
            "Training loss is: 1.720102081044515\n",
            "Training accuracy is: 0.7515\n",
            "Validation loss is: 1.7087314128875732\n",
            "Validation accuracy is: 0.7547\n",
            "L2 norm is 142.33228397369385 \n",
            "\n",
            "Epoch 14 finished.\n",
            "Training loss is: 1.7110954750061036\n",
            "Training accuracy is: 0.75555\n",
            "Validation loss is: 1.7049109935760498\n",
            "Validation accuracy is: 0.7601\n",
            "L2 norm is 142.41872835159302 \n",
            "\n",
            "Epoch 15 finished.\n",
            "Training loss is: 1.7054203893025717\n",
            "Training accuracy is: 0.7624833333333333\n",
            "Validation loss is: 1.698136329650879\n",
            "Validation accuracy is: 0.7649\n",
            "L2 norm is 142.48838567733765 \n",
            "\n",
            "Epoch 16 finished.\n",
            "Training loss is: 1.7007265419006348\n",
            "Training accuracy is: 0.7634333333333333\n",
            "Validation loss is: 1.6973549127578735\n",
            "Validation accuracy is: 0.7649\n",
            "L2 norm is 142.54672265052795 \n",
            "\n",
            "Epoch 17 finished.\n",
            "Training loss is: 1.6975089421590168\n",
            "Training accuracy is: 0.7655\n",
            "Validation loss is: 1.6959549188613892\n",
            "Validation accuracy is: 0.7661\n",
            "L2 norm is 142.5995168685913 \n",
            "\n",
            "Epoch 18 finished.\n",
            "Training loss is: 1.6942366775512696\n",
            "Training accuracy is: 0.7710166666666667\n",
            "Validation loss is: 1.690798044204712\n",
            "Validation accuracy is: 0.7717\n",
            "L2 norm is 142.64734148979187 \n",
            "\n",
            "Epoch 19 finished.\n",
            "Training loss is: 1.6920197240193684\n",
            "Training accuracy is: 0.7732666666666667\n",
            "Validation loss is: 1.6881009340286255\n",
            "Validation accuracy is: 0.7739\n",
            "L2 norm is 142.69147539138794 \n",
            "\n",
            "Epoch 20 finished.\n",
            "Training loss is: 1.6896511049906413\n",
            "Training accuracy is: 0.7759\n",
            "Validation loss is: 1.6858546733856201\n",
            "Validation accuracy is: 0.7759\n",
            "L2 norm is 142.73190069198608 \n",
            "\n",
            "Epoch 21 finished.\n",
            "Training loss is: 1.6873746037801107\n",
            "Training accuracy is: 0.7750833333333333\n",
            "Validation loss is: 1.6869416236877441\n",
            "Validation accuracy is: 0.7742\n",
            "L2 norm is 142.7695517539978 \n",
            "\n",
            "Epoch 22 finished.\n",
            "Training loss is: 1.6856619336446126\n",
            "Training accuracy is: 0.7791166666666667\n",
            "Validation loss is: 1.6829155683517456\n",
            "Validation accuracy is: 0.778\n",
            "L2 norm is 142.80545258522034 \n",
            "\n",
            "Epoch 23 finished.\n",
            "Training loss is: 1.6840942392985025\n",
            "Training accuracy is: 0.7799666666666667\n",
            "Validation loss is: 1.6823962926864624\n",
            "Validation accuracy is: 0.7793\n",
            "L2 norm is 142.83938121795654 \n",
            "\n",
            "Epoch 24 finished.\n",
            "Training loss is: 1.682423459370931\n",
            "Training accuracy is: 0.7819833333333334\n",
            "Validation loss is: 1.6810168027877808\n",
            "Validation accuracy is: 0.7796\n",
            "L2 norm is 142.8723475933075 \n",
            "\n",
            "Epoch 25 finished.\n",
            "Training loss is: 1.680818379720052\n",
            "Training accuracy is: 0.78325\n",
            "Validation loss is: 1.6793873310089111\n",
            "Validation accuracy is: 0.7815\n",
            "L2 norm is 142.90270948410034 \n",
            "\n",
            "Epoch 26 finished.\n",
            "Training loss is: 1.6793556027730305\n",
            "Training accuracy is: 0.7840833333333334\n",
            "Validation loss is: 1.6790423393249512\n",
            "Validation accuracy is: 0.781\n",
            "L2 norm is 142.93259596824646 \n",
            "\n",
            "Epoch 27 finished.\n",
            "Training loss is: 1.678283127339681\n",
            "Training accuracy is: 0.78375\n",
            "Validation loss is: 1.6794993877410889\n",
            "Validation accuracy is: 0.7817\n",
            "L2 norm is 142.96018886566162 \n",
            "\n",
            "Epoch 28 finished.\n",
            "Training loss is: 1.6769110911051432\n",
            "Training accuracy is: 0.78675\n",
            "Validation loss is: 1.6767476797103882\n",
            "Validation accuracy is: 0.7843\n",
            "L2 norm is 142.98770380020142 \n",
            "\n",
            "Epoch 29 finished.\n",
            "Training loss is: 1.6756246119181315\n",
            "Training accuracy is: 0.7874333333333333\n",
            "Validation loss is: 1.676313042640686\n",
            "Validation accuracy is: 0.7842\n",
            "L2 norm is 143.01562786102295 \n",
            "\n",
            "Epoch 30 finished.\n",
            "Training loss is: 1.6745480072021484\n",
            "Training accuracy is: 0.7853333333333333\n",
            "Validation loss is: 1.6783783435821533\n",
            "Validation accuracy is: 0.7813\n",
            "L2 norm is 143.04090356826782 \n",
            "\n",
            "Epoch 31 finished.\n",
            "Training loss is: 1.6736382614135743\n",
            "Training accuracy is: 0.79025\n",
            "Validation loss is: 1.6745017766952515\n",
            "Validation accuracy is: 0.7855\n",
            "L2 norm is 143.06642889976501 \n",
            "\n",
            "Epoch 32 finished.\n",
            "Training loss is: 1.6721701784769694\n",
            "Training accuracy is: 0.7902833333333333\n",
            "Validation loss is: 1.6741576194763184\n",
            "Validation accuracy is: 0.7861\n",
            "L2 norm is 143.09271812438965 \n",
            "\n",
            "Epoch 33 finished.\n",
            "Training loss is: 1.6715120051066081\n",
            "Training accuracy is: 0.7915166666666666\n",
            "Validation loss is: 1.6731278896331787\n",
            "Validation accuracy is: 0.787\n",
            "L2 norm is 143.11611938476562 \n",
            "\n",
            "Epoch 34 finished.\n",
            "Training loss is: 1.6704887639363606\n",
            "Training accuracy is: 0.7928\n",
            "Validation loss is: 1.6725980043411255\n",
            "Validation accuracy is: 0.7875\n",
            "L2 norm is 143.13956260681152 \n",
            "\n",
            "Epoch 35 finished.\n",
            "Training loss is: 1.6698134012858072\n",
            "Training accuracy is: 0.79275\n",
            "Validation loss is: 1.6730144023895264\n",
            "Validation accuracy is: 0.7867\n",
            "L2 norm is 143.1612331867218 \n",
            "\n",
            "Epoch 36 finished.\n",
            "Training loss is: 1.6688835202534993\n",
            "Training accuracy is: 0.7934666666666667\n",
            "Validation loss is: 1.6722660064697266\n",
            "Validation accuracy is: 0.7876\n",
            "L2 norm is 143.1830427646637 \n",
            "\n",
            "Epoch 37 finished.\n",
            "Training loss is: 1.6678558184305827\n",
            "Training accuracy is: 0.7935\n",
            "Validation loss is: 1.6722371578216553\n",
            "Validation accuracy is: 0.7871\n",
            "L2 norm is 143.20361971855164 \n",
            "\n",
            "Epoch 38 finished.\n",
            "Training loss is: 1.6669055890401205\n",
            "Training accuracy is: 0.7948333333333333\n",
            "Validation loss is: 1.6710072755813599\n",
            "Validation accuracy is: 0.7884\n",
            "L2 norm is 143.22391963005066 \n",
            "\n",
            "Epoch 39 finished.\n",
            "Training loss is: 1.6662007016499838\n",
            "Training accuracy is: 0.7959833333333334\n",
            "Validation loss is: 1.6681898832321167\n",
            "Validation accuracy is: 0.7894\n",
            "L2 norm is 143.2394733428955 \n",
            "\n",
            "Epoch 40 finished.\n",
            "Training loss is: 1.6010369756062826\n",
            "Training accuracy is: 0.8602\n",
            "Validation loss is: 1.6115756034851074\n",
            "Validation accuracy is: 0.8546\n",
            "L2 norm is 143.32394552230835 \n",
            "\n",
            "Epoch 41 finished.\n",
            "Training loss is: 1.5900628954569498\n",
            "Training accuracy is: 0.8762166666666666\n",
            "Validation loss is: 1.5933663845062256\n",
            "Validation accuracy is: 0.8693\n",
            "L2 norm is 143.38746452331543 \n",
            "\n",
            "Epoch 42 finished.\n",
            "Training loss is: 1.551828224690755\n",
            "Training accuracy is: 0.9429666666666666\n",
            "Validation loss is: 1.530996561050415\n",
            "Validation accuracy is: 0.9358\n",
            "L2 norm is 143.5105881690979 \n",
            "\n",
            "Epoch 43 finished.\n",
            "Training loss is: 1.52493169631958\n",
            "Training accuracy is: 0.9449666666666666\n",
            "Validation loss is: 1.5304607152938843\n",
            "Validation accuracy is: 0.9359\n",
            "L2 norm is 143.61010551452637 \n",
            "\n",
            "Epoch 44 finished.\n",
            "Training loss is: 1.5158614382425943\n",
            "Training accuracy is: 0.9548166666666666\n",
            "Validation loss is: 1.517759084701538\n",
            "Validation accuracy is: 0.9466\n",
            "L2 norm is 143.69181871414185 \n",
            "\n",
            "Epoch 45 finished.\n",
            "Training loss is: 1.5100186681111654\n",
            "Training accuracy is: 0.9564333333333334\n",
            "Validation loss is: 1.5165419578552246\n",
            "Validation accuracy is: 0.9471\n",
            "L2 norm is 143.76252508163452 \n",
            "\n",
            "Epoch 46 finished.\n",
            "Training loss is: 1.506621319325765\n",
            "Training accuracy is: 0.9633166666666667\n",
            "Validation loss is: 1.511242389678955\n",
            "Validation accuracy is: 0.9524\n",
            "L2 norm is 143.8265528678894 \n",
            "\n",
            "Epoch 47 finished.\n",
            "Training loss is: 1.502819925181071\n",
            "Training accuracy is: 0.96695\n",
            "Validation loss is: 1.506910800933838\n",
            "Validation accuracy is: 0.9572\n",
            "L2 norm is 143.88462781906128 \n",
            "\n",
            "Epoch 48 finished.\n",
            "Training loss is: 1.4998047154744467\n",
            "Training accuracy is: 0.97065\n",
            "Validation loss is: 1.5051333904266357\n",
            "Validation accuracy is: 0.9563\n",
            "L2 norm is 143.9385223388672 \n",
            "\n",
            "Epoch 49 finished.\n",
            "Training loss is: 1.4973587567647297\n",
            "Training accuracy is: 0.9718\n",
            "Validation loss is: 1.5032652616500854\n",
            "Validation accuracy is: 0.9608\n",
            "L2 norm is 143.98940706253052 \n",
            "\n",
            "Epoch 50 finished.\n",
            "Training loss is: 1.4952315002441405\n",
            "Training accuracy is: 0.9723666666666667\n",
            "Validation loss is: 1.5033683776855469\n",
            "Validation accuracy is: 0.9593\n",
            "L2 norm is 144.03705501556396 \n",
            "\n",
            "Epoch 51 finished.\n",
            "Training loss is: 1.4932379880269369\n",
            "Training accuracy is: 0.97515\n",
            "Validation loss is: 1.5005193948745728\n",
            "Validation accuracy is: 0.9629\n",
            "L2 norm is 144.08112716674805 \n",
            "\n",
            "Epoch 52 finished.\n",
            "Training loss is: 1.49132572555542\n",
            "Training accuracy is: 0.97425\n",
            "Validation loss is: 1.5010658502578735\n",
            "Validation accuracy is: 0.9617\n",
            "L2 norm is 144.12328338623047 \n",
            "\n",
            "Epoch 53 finished.\n",
            "Training loss is: 1.4901013982137044\n",
            "Training accuracy is: 0.97195\n",
            "Validation loss is: 1.504889726638794\n",
            "Validation accuracy is: 0.9574\n",
            "L2 norm is 144.16288042068481 \n",
            "\n",
            "Epoch 54 finished.\n",
            "Training loss is: 1.4887199073791504\n",
            "Training accuracy is: 0.9774333333333334\n",
            "Validation loss is: 1.4997501373291016\n",
            "Validation accuracy is: 0.9631\n",
            "L2 norm is 144.1998987197876 \n",
            "\n",
            "Epoch 55 finished.\n",
            "Training loss is: 1.4872596972147625\n",
            "Training accuracy is: 0.97425\n",
            "Validation loss is: 1.502687931060791\n",
            "Validation accuracy is: 0.9606\n",
            "L2 norm is 144.23609495162964 \n",
            "\n",
            "Epoch 56 finished.\n",
            "Training loss is: 1.486040042368571\n",
            "Training accuracy is: 0.9791666666666666\n",
            "Validation loss is: 1.4997323751449585\n",
            "Validation accuracy is: 0.9625\n",
            "L2 norm is 144.2705202102661 \n",
            "\n",
            "Epoch 57 finished.\n",
            "Training loss is: 1.4855476048787435\n",
            "Training accuracy is: 0.9787333333333333\n",
            "Validation loss is: 1.4997339248657227\n",
            "Validation accuracy is: 0.9619\n",
            "L2 norm is 144.3044729232788 \n",
            "\n",
            "Epoch 58 finished.\n",
            "Training loss is: 1.484342713165283\n",
            "Training accuracy is: 0.9785\n",
            "Validation loss is: 1.5017921924591064\n",
            "Validation accuracy is: 0.9608\n",
            "L2 norm is 144.3356170654297 \n",
            "\n",
            "Epoch 59 finished.\n",
            "Training loss is: 1.48357090326945\n",
            "Training accuracy is: 0.9794833333333334\n",
            "Validation loss is: 1.5007396936416626\n",
            "Validation accuracy is: 0.9614\n",
            "L2 norm is 144.3655047416687 \n",
            "\n",
            "Epoch 60 finished.\n",
            "Training loss is: 1.4824206301371257\n",
            "Training accuracy is: 0.9821333333333333\n",
            "Validation loss is: 1.4971915483474731\n",
            "Validation accuracy is: 0.9652\n",
            "L2 norm is 144.3947606086731 \n",
            "\n",
            "Epoch 61 finished.\n",
            "Training loss is: 1.482004508972168\n",
            "Training accuracy is: 0.9835333333333334\n",
            "Validation loss is: 1.4964832067489624\n",
            "Validation accuracy is: 0.9654\n",
            "L2 norm is 144.42272520065308 \n",
            "\n",
            "Epoch 62 finished.\n",
            "Training loss is: 1.48134593073527\n",
            "Training accuracy is: 0.9839833333333333\n",
            "Validation loss is: 1.4961012601852417\n",
            "Validation accuracy is: 0.9652\n",
            "L2 norm is 144.44973278045654 \n",
            "\n",
            "Epoch 63 finished.\n",
            "Training loss is: 1.4810725410461425\n",
            "Training accuracy is: 0.9838666666666667\n",
            "Validation loss is: 1.496060848236084\n",
            "Validation accuracy is: 0.9652\n",
            "L2 norm is 144.47556495666504 \n",
            "\n",
            "Epoch 64 finished.\n",
            "Training loss is: 1.4799501736958822\n",
            "Training accuracy is: 0.9848166666666667\n",
            "Validation loss is: 1.4951456785202026\n",
            "Validation accuracy is: 0.9672\n",
            "L2 norm is 144.49964427947998 \n",
            "\n",
            "Epoch 65 finished.\n",
            "Training loss is: 1.4795633786519369\n",
            "Training accuracy is: 0.9847833333333333\n",
            "Validation loss is: 1.4956318140029907\n",
            "Validation accuracy is: 0.966\n",
            "L2 norm is 144.52209615707397 \n",
            "\n",
            "Epoch 66 finished.\n",
            "Training loss is: 1.4790485186258953\n",
            "Training accuracy is: 0.9843833333333334\n",
            "Validation loss is: 1.496677279472351\n",
            "Validation accuracy is: 0.9648\n",
            "L2 norm is 144.54364824295044 \n",
            "\n",
            "Epoch 67 finished.\n",
            "Training loss is: 1.4785119669596354\n",
            "Training accuracy is: 0.9849166666666667\n",
            "Validation loss is: 1.495981216430664\n",
            "Validation accuracy is: 0.9664\n",
            "L2 norm is 144.56416845321655 \n",
            "\n",
            "Epoch 68 finished.\n",
            "Training loss is: 1.4780567159016926\n",
            "Training accuracy is: 0.9858166666666667\n",
            "Validation loss is: 1.4946733713150024\n",
            "Validation accuracy is: 0.9673\n",
            "L2 norm is 144.58351707458496 \n",
            "\n",
            "Epoch 69 finished.\n",
            "Training loss is: 1.477569300587972\n",
            "Training accuracy is: 0.98495\n",
            "Validation loss is: 1.4961274862289429\n",
            "Validation accuracy is: 0.9655\n",
            "L2 norm is 144.601469039917 \n",
            "\n",
            "Epoch 70 finished.\n",
            "Training loss is: 1.4772361956278484\n",
            "Training accuracy is: 0.9859333333333333\n",
            "Validation loss is: 1.4950389862060547\n",
            "Validation accuracy is: 0.9669\n",
            "L2 norm is 144.61904573440552 \n",
            "\n",
            "Epoch 71 finished.\n",
            "Training loss is: 1.4770248140970865\n",
            "Training accuracy is: 0.9864\n",
            "Validation loss is: 1.494912028312683\n",
            "Validation accuracy is: 0.9671\n",
            "L2 norm is 144.6360216140747 \n",
            "\n",
            "Epoch 72 finished.\n",
            "Training loss is: 1.476854071553548\n",
            "Training accuracy is: 0.9863833333333333\n",
            "Validation loss is: 1.4949569702148438\n",
            "Validation accuracy is: 0.9667\n",
            "L2 norm is 144.65294361114502 \n",
            "\n",
            "Epoch 73 finished.\n",
            "Training loss is: 1.4765535001118977\n",
            "Training accuracy is: 0.9867\n",
            "Validation loss is: 1.494131088256836\n",
            "Validation accuracy is: 0.9678\n",
            "L2 norm is 144.66907739639282 \n",
            "\n",
            "Epoch 74 finished.\n",
            "Training loss is: 1.476053687286377\n",
            "Training accuracy is: 0.98695\n",
            "Validation loss is: 1.4938745498657227\n",
            "Validation accuracy is: 0.9671\n",
            "L2 norm is 144.68397998809814 \n",
            "\n",
            "Epoch 75 finished.\n",
            "Training loss is: 1.4758582954406738\n",
            "Training accuracy is: 0.9868666666666667\n",
            "Validation loss is: 1.4941611289978027\n",
            "Validation accuracy is: 0.9675\n",
            "L2 norm is 144.69768285751343 \n",
            "\n",
            "Epoch 76 finished.\n",
            "Training loss is: 1.475745427195231\n",
            "Training accuracy is: 0.9872833333333333\n",
            "Validation loss is: 1.494143009185791\n",
            "Validation accuracy is: 0.9677\n",
            "L2 norm is 144.7111439704895 \n",
            "\n",
            "Epoch 77 finished.\n",
            "Training loss is: 1.4756417922973633\n",
            "Training accuracy is: 0.98725\n",
            "Validation loss is: 1.494001865386963\n",
            "Validation accuracy is: 0.9673\n",
            "L2 norm is 144.72418880462646 \n",
            "\n",
            "Epoch 78 finished.\n",
            "Training loss is: 1.4753889246622722\n",
            "Training accuracy is: 0.98655\n",
            "Validation loss is: 1.495300054550171\n",
            "Validation accuracy is: 0.9661\n",
            "L2 norm is 144.7366042137146 \n",
            "\n",
            "Epoch 79 finished.\n",
            "Training loss is: 1.4749934511820475\n",
            "Training accuracy is: 0.98765\n",
            "Validation loss is: 1.4939500093460083\n",
            "Validation accuracy is: 0.9669\n",
            "L2 norm is 144.74736881256104 \n",
            "\n",
            "Epoch 80 finished.\n",
            "Training loss is: 1.4748400891621907\n",
            "Training accuracy is: 0.98765\n",
            "Validation loss is: 1.494271993637085\n",
            "Validation accuracy is: 0.9673\n",
            "L2 norm is 144.7578649520874 \n",
            "\n",
            "Epoch 81 finished.\n",
            "Training loss is: 1.4747826021830241\n",
            "Training accuracy is: 0.9878833333333333\n",
            "Validation loss is: 1.4940253496170044\n",
            "Validation accuracy is: 0.9672\n",
            "L2 norm is 144.7687497138977 \n",
            "\n",
            "Epoch 82 finished.\n",
            "Training loss is: 1.474565608215332\n",
            "Training accuracy is: 0.9879666666666667\n",
            "Validation loss is: 1.4930930137634277\n",
            "Validation accuracy is: 0.9681\n",
            "L2 norm is 144.77920150756836 \n",
            "\n",
            "Epoch 83 finished.\n",
            "Training loss is: 1.4745397300720215\n",
            "Training accuracy is: 0.9880833333333333\n",
            "Validation loss is: 1.4934000968933105\n",
            "Validation accuracy is: 0.9677\n",
            "L2 norm is 144.7894630432129 \n",
            "\n",
            "Epoch 84 finished.\n",
            "Training loss is: 1.474359964243571\n",
            "Training accuracy is: 0.9882\n",
            "Validation loss is: 1.4935424327850342\n",
            "Validation accuracy is: 0.9675\n",
            "L2 norm is 144.79894161224365 \n",
            "\n",
            "Epoch 85 finished.\n",
            "Training loss is: 1.4742014556884766\n",
            "Training accuracy is: 0.9882166666666666\n",
            "Validation loss is: 1.4934489727020264\n",
            "Validation accuracy is: 0.9682\n",
            "L2 norm is 144.8078842163086 \n",
            "\n",
            "Epoch 86 finished.\n",
            "Training loss is: 1.474139826965332\n",
            "Training accuracy is: 0.98835\n",
            "Validation loss is: 1.4933329820632935\n",
            "Validation accuracy is: 0.9678\n",
            "L2 norm is 144.8167748451233 \n",
            "\n",
            "Epoch 87 finished.\n",
            "Training loss is: 1.4741025652567545\n",
            "Training accuracy is: 0.9883666666666666\n",
            "Validation loss is: 1.4934121370315552\n",
            "Validation accuracy is: 0.9677\n",
            "L2 norm is 144.8251986503601 \n",
            "\n",
            "Epoch 88 finished.\n",
            "Training loss is: 1.473822511291504\n",
            "Training accuracy is: 0.9883666666666666\n",
            "Validation loss is: 1.4930005073547363\n",
            "Validation accuracy is: 0.9685\n",
            "L2 norm is 144.83248138427734 \n",
            "\n",
            "Epoch 89 finished.\n",
            "Training loss is: 1.473746260325114\n",
            "Training accuracy is: 0.98845\n",
            "Validation loss is: 1.4930119514465332\n",
            "Validation accuracy is: 0.9684\n",
            "L2 norm is 144.8390703201294 \n",
            "\n",
            "Epoch 90 finished.\n",
            "Training loss is: 1.4738321184794108\n",
            "Training accuracy is: 0.9885166666666667\n",
            "Validation loss is: 1.4930661916732788\n",
            "Validation accuracy is: 0.9677\n",
            "L2 norm is 144.84647035598755 \n",
            "\n",
            "Epoch 91 finished.\n",
            "Training loss is: 1.4736644104003906\n",
            "Training accuracy is: 0.98855\n",
            "Validation loss is: 1.4933336973190308\n",
            "Validation accuracy is: 0.9672\n",
            "L2 norm is 144.85315322875977 \n",
            "\n",
            "Epoch 92 finished.\n",
            "Training loss is: 1.4736365414937338\n",
            "Training accuracy is: 0.9885833333333334\n",
            "Validation loss is: 1.493944525718689\n",
            "Validation accuracy is: 0.9669\n",
            "L2 norm is 144.8598222732544 \n",
            "\n",
            "Epoch 93 finished.\n",
            "Training loss is: 1.4735724090576172\n",
            "Training accuracy is: 0.9887333333333334\n",
            "Validation loss is: 1.4929804801940918\n",
            "Validation accuracy is: 0.9679\n",
            "L2 norm is 144.866783618927 \n",
            "\n",
            "Epoch 94 finished.\n",
            "Training loss is: 1.4734152135213217\n",
            "Training accuracy is: 0.9887333333333334\n",
            "Validation loss is: 1.4928407669067383\n",
            "Validation accuracy is: 0.9678\n",
            "L2 norm is 144.8730206489563 \n",
            "\n",
            "Epoch 95 finished.\n",
            "Training loss is: 1.473386720530192\n",
            "Training accuracy is: 0.9887666666666667\n",
            "Validation loss is: 1.4926506280899048\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 144.87869119644165 \n",
            "\n",
            "Epoch 96 finished.\n",
            "Training loss is: 1.4733226483662922\n",
            "Training accuracy is: 0.9888\n",
            "Validation loss is: 1.492856502532959\n",
            "Validation accuracy is: 0.9684\n",
            "L2 norm is 144.8843126296997 \n",
            "\n",
            "Epoch 97 finished.\n",
            "Training loss is: 1.4733709645589192\n",
            "Training accuracy is: 0.98885\n",
            "Validation loss is: 1.4930404424667358\n",
            "Validation accuracy is: 0.9681\n",
            "L2 norm is 144.8904571533203 \n",
            "\n",
            "Epoch 98 finished.\n",
            "Training loss is: 1.4732480410257975\n",
            "Training accuracy is: 0.9889\n",
            "Validation loss is: 1.4928597211837769\n",
            "Validation accuracy is: 0.9683\n",
            "L2 norm is 144.89617919921875 \n",
            "\n",
            "Epoch 99 finished.\n",
            "Training loss is: 1.4731856765747071\n",
            "Training accuracy is: 0.9889333333333333\n",
            "Validation loss is: 1.4925756454467773\n",
            "Validation accuracy is: 0.9686\n",
            "L2 norm is 144.90153312683105 \n",
            "\n",
            "Epoch 100 finished.\n",
            "Training loss is: 1.4731331797281901\n",
            "Training accuracy is: 0.9889833333333333\n",
            "Validation loss is: 1.4928022623062134\n",
            "Validation accuracy is: 0.9681\n",
            "L2 norm is 144.90667819976807 \n",
            "\n",
            "Epoch 101 finished.\n",
            "Training loss is: 1.4731235743204754\n",
            "Training accuracy is: 0.9890166666666667\n",
            "Validation loss is: 1.4925661087036133\n",
            "Validation accuracy is: 0.9682\n",
            "L2 norm is 144.911940574646 \n",
            "\n",
            "Epoch 102 finished.\n",
            "Training loss is: 1.4731231218973795\n",
            "Training accuracy is: 0.98905\n",
            "Validation loss is: 1.4926886558532715\n",
            "Validation accuracy is: 0.968\n",
            "L2 norm is 144.91733503341675 \n",
            "\n",
            "Epoch 103 finished.\n",
            "Training loss is: 1.473111671447754\n",
            "Training accuracy is: 0.98905\n",
            "Validation loss is: 1.4928480386734009\n",
            "Validation accuracy is: 0.9683\n",
            "L2 norm is 144.922700881958 \n",
            "\n",
            "Epoch 104 finished.\n",
            "Training loss is: 1.4729799395243326\n",
            "Training accuracy is: 0.9890833333333333\n",
            "Validation loss is: 1.4926559925079346\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 144.92747592926025 \n",
            "\n",
            "Epoch 105 finished.\n",
            "Training loss is: 1.4733100682576497\n",
            "Training accuracy is: 0.9891166666666666\n",
            "Validation loss is: 1.492630124092102\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 144.9330654144287 \n",
            "\n",
            "Epoch 106 finished.\n",
            "Training loss is: 1.4729490402221679\n",
            "Training accuracy is: 0.98915\n",
            "Validation loss is: 1.4927823543548584\n",
            "Validation accuracy is: 0.9685\n",
            "L2 norm is 144.93779182434082 \n",
            "\n",
            "Epoch 107 finished.\n",
            "Training loss is: 1.4728792078653972\n",
            "Training accuracy is: 0.9891666666666666\n",
            "Validation loss is: 1.492477536201477\n",
            "Validation accuracy is: 0.9687\n",
            "L2 norm is 144.94228410720825 \n",
            "\n",
            "Epoch 108 finished.\n",
            "Training loss is: 1.4728650825500489\n",
            "Training accuracy is: 0.9891666666666666\n",
            "Validation loss is: 1.4923955202102661\n",
            "Validation accuracy is: 0.9685\n",
            "L2 norm is 144.94655466079712 \n",
            "\n",
            "Epoch 109 finished.\n",
            "Training loss is: 1.472819297281901\n",
            "Training accuracy is: 0.9891666666666666\n",
            "Validation loss is: 1.4924474954605103\n",
            "Validation accuracy is: 0.9684\n",
            "L2 norm is 144.95046854019165 \n",
            "\n",
            "Epoch 110 finished.\n",
            "Training loss is: 1.4728030934651692\n",
            "Training accuracy is: 0.9892\n",
            "Validation loss is: 1.4924265146255493\n",
            "Validation accuracy is: 0.9686\n",
            "L2 norm is 144.95417261123657 \n",
            "\n",
            "Epoch 111 finished.\n",
            "Training loss is: 1.4728279912312825\n",
            "Training accuracy is: 0.9892166666666666\n",
            "Validation loss is: 1.4925516843795776\n",
            "Validation accuracy is: 0.9685\n",
            "L2 norm is 144.95801401138306 \n",
            "\n",
            "Epoch 112 finished.\n",
            "Training loss is: 1.4727833541870117\n",
            "Training accuracy is: 0.9892333333333333\n",
            "Validation loss is: 1.4924559593200684\n",
            "Validation accuracy is: 0.9687\n",
            "L2 norm is 144.9619016647339 \n",
            "\n",
            "Epoch 113 finished.\n",
            "Training loss is: 1.4727537963867188\n",
            "Training accuracy is: 0.9892333333333333\n",
            "Validation loss is: 1.4923099279403687\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 144.9655842781067 \n",
            "\n",
            "Epoch 114 finished.\n",
            "Training loss is: 1.4727556800842285\n",
            "Training accuracy is: 0.9892666666666666\n",
            "Validation loss is: 1.4924516677856445\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 144.9693145751953 \n",
            "\n",
            "Epoch 115 finished.\n",
            "Training loss is: 1.472720947519938\n",
            "Training accuracy is: 0.9893\n",
            "Validation loss is: 1.4928548336029053\n",
            "Validation accuracy is: 0.9683\n",
            "L2 norm is 144.97277784347534 \n",
            "\n",
            "Epoch 116 finished.\n",
            "Training loss is: 1.4726761622111002\n",
            "Training accuracy is: 0.9893\n",
            "Validation loss is: 1.4924452304840088\n",
            "Validation accuracy is: 0.9687\n",
            "L2 norm is 144.97629737854004 \n",
            "\n",
            "Epoch 117 finished.\n",
            "Training loss is: 1.4726678479512532\n",
            "Training accuracy is: 0.9893166666666666\n",
            "Validation loss is: 1.4924871921539307\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 144.97970342636108 \n",
            "\n",
            "Epoch 118 finished.\n",
            "Training loss is: 1.4726620971679687\n",
            "Training accuracy is: 0.9893333333333333\n",
            "Validation loss is: 1.4926241636276245\n",
            "Validation accuracy is: 0.9683\n",
            "L2 norm is 144.98318672180176 \n",
            "\n",
            "Epoch 119 finished.\n",
            "Training loss is: 1.47262199122111\n",
            "Training accuracy is: 0.9893333333333333\n",
            "Validation loss is: 1.4925847053527832\n",
            "Validation accuracy is: 0.9685\n",
            "L2 norm is 144.98645544052124 \n",
            "\n",
            "Epoch 120 finished.\n",
            "Training loss is: 1.472606411997477\n",
            "Training accuracy is: 0.98935\n",
            "Validation loss is: 1.4923888444900513\n",
            "Validation accuracy is: 0.9682\n",
            "L2 norm is 144.9896535873413 \n",
            "\n",
            "Epoch 121 finished.\n",
            "Training loss is: 1.4726085978190104\n",
            "Training accuracy is: 0.9893833333333333\n",
            "Validation loss is: 1.4924331903457642\n",
            "Validation accuracy is: 0.9686\n",
            "L2 norm is 144.99283409118652 \n",
            "\n",
            "Epoch 122 finished.\n",
            "Training loss is: 1.4725941869099934\n",
            "Training accuracy is: 0.9894166666666667\n",
            "Validation loss is: 1.4924637079238892\n",
            "Validation accuracy is: 0.9691\n",
            "L2 norm is 144.9962019920349 \n",
            "\n",
            "Epoch 123 finished.\n",
            "Training loss is: 1.4725492403666178\n",
            "Training accuracy is: 0.9894166666666667\n",
            "Validation loss is: 1.4923902750015259\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 144.9993257522583 \n",
            "\n",
            "Epoch 124 finished.\n",
            "Training loss is: 1.4725292681376139\n",
            "Training accuracy is: 0.9894166666666667\n",
            "Validation loss is: 1.4925754070281982\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 145.00236415863037 \n",
            "\n",
            "Epoch 125 finished.\n",
            "Training loss is: 1.4725281776428223\n",
            "Training accuracy is: 0.9894333333333334\n",
            "Validation loss is: 1.4924405813217163\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 145.00532150268555 \n",
            "\n",
            "Epoch 126 finished.\n",
            "Training loss is: 1.472611267344157\n",
            "Training accuracy is: 0.9894666666666667\n",
            "Validation loss is: 1.4924967288970947\n",
            "Validation accuracy is: 0.9689\n",
            "L2 norm is 145.00863122940063 \n",
            "\n",
            "Epoch 127 finished.\n",
            "Training loss is: 1.4724967280069987\n",
            "Training accuracy is: 0.9894666666666667\n",
            "Validation loss is: 1.4925150871276855\n",
            "Validation accuracy is: 0.9687\n",
            "L2 norm is 145.01177597045898 \n",
            "\n",
            "Epoch 128 finished.\n",
            "Training loss is: 1.4724586329142253\n",
            "Training accuracy is: 0.9894833333333334\n",
            "Validation loss is: 1.4925775527954102\n",
            "Validation accuracy is: 0.9686\n",
            "L2 norm is 145.0146541595459 \n",
            "\n",
            "Epoch 129 finished.\n",
            "Training loss is: 1.4724550819396973\n",
            "Training accuracy is: 0.9895\n",
            "Validation loss is: 1.4927663803100586\n",
            "Validation accuracy is: 0.9684\n",
            "L2 norm is 145.01749229431152 \n",
            "\n",
            "Epoch 130 finished.\n",
            "Training loss is: 1.4724356803894043\n",
            "Training accuracy is: 0.9895166666666667\n",
            "Validation loss is: 1.49247407913208\n",
            "Validation accuracy is: 0.9687\n",
            "L2 norm is 145.02040910720825 \n",
            "\n",
            "Epoch 131 finished.\n",
            "Training loss is: 1.4724137079874675\n",
            "Training accuracy is: 0.9895333333333334\n",
            "Validation loss is: 1.4925298690795898\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 145.02320766448975 \n",
            "\n",
            "Epoch 132 finished.\n",
            "Training loss is: 1.4723940205891928\n",
            "Training accuracy is: 0.9895333333333334\n",
            "Validation loss is: 1.4924269914627075\n",
            "Validation accuracy is: 0.9686\n",
            "L2 norm is 145.025945186615 \n",
            "\n",
            "Epoch 133 finished.\n",
            "Training loss is: 1.472381672668457\n",
            "Training accuracy is: 0.9895333333333334\n",
            "Validation loss is: 1.4924795627593994\n",
            "Validation accuracy is: 0.9693\n",
            "L2 norm is 145.02855110168457 \n",
            "\n",
            "Epoch 134 finished.\n",
            "Training loss is: 1.4723832293192545\n",
            "Training accuracy is: 0.98955\n",
            "Validation loss is: 1.4924161434173584\n",
            "Validation accuracy is: 0.9693\n",
            "L2 norm is 145.03109550476074 \n",
            "\n",
            "Epoch 135 finished.\n",
            "Training loss is: 1.4723711168924967\n",
            "Training accuracy is: 0.98955\n",
            "Validation loss is: 1.4924201965332031\n",
            "Validation accuracy is: 0.9689\n",
            "L2 norm is 145.03368425369263 \n",
            "\n",
            "Epoch 136 finished.\n",
            "Training loss is: 1.4723617223103842\n",
            "Training accuracy is: 0.98955\n",
            "Validation loss is: 1.4923973083496094\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.0361123085022 \n",
            "\n",
            "Epoch 137 finished.\n",
            "Training loss is: 1.4723577087402344\n",
            "Training accuracy is: 0.98955\n",
            "Validation loss is: 1.4925192594528198\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 145.03847742080688 \n",
            "\n",
            "Epoch 138 finished.\n",
            "Training loss is: 1.4723487981160481\n",
            "Training accuracy is: 0.98955\n",
            "Validation loss is: 1.492458701133728\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 145.0407910346985 \n",
            "\n",
            "Epoch 139 finished.\n",
            "Training loss is: 1.4723575378417968\n",
            "Training accuracy is: 0.9895666666666667\n",
            "Validation loss is: 1.4924417734146118\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 145.04309463500977 \n",
            "\n",
            "Epoch 140 finished.\n",
            "Training loss is: 1.4723390963236491\n",
            "Training accuracy is: 0.9895833333333334\n",
            "Validation loss is: 1.492531418800354\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.04544734954834 \n",
            "\n",
            "Epoch 141 finished.\n",
            "Training loss is: 1.4723419525146484\n",
            "Training accuracy is: 0.9896166666666667\n",
            "Validation loss is: 1.4925568103790283\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 145.0477933883667 \n",
            "\n",
            "Epoch 142 finished.\n",
            "Training loss is: 1.4723213656107585\n",
            "Training accuracy is: 0.9896333333333334\n",
            "Validation loss is: 1.4923005104064941\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.05035877227783 \n",
            "\n",
            "Epoch 143 finished.\n",
            "Training loss is: 1.4723265904744467\n",
            "Training accuracy is: 0.9896666666666667\n",
            "Validation loss is: 1.4926458597183228\n",
            "Validation accuracy is: 0.9684\n",
            "L2 norm is 145.05315351486206 \n",
            "\n",
            "Epoch 144 finished.\n",
            "Training loss is: 1.4722499972025553\n",
            "Training accuracy is: 0.9896666666666667\n",
            "Validation loss is: 1.4926230907440186\n",
            "Validation accuracy is: 0.9689\n",
            "L2 norm is 145.05558919906616 \n",
            "\n",
            "Epoch 145 finished.\n",
            "Training loss is: 1.4722384086608886\n",
            "Training accuracy is: 0.9897\n",
            "Validation loss is: 1.4923189878463745\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 145.0579285621643 \n",
            "\n",
            "Epoch 146 finished.\n",
            "Training loss is: 1.4722262217203776\n",
            "Training accuracy is: 0.9897\n",
            "Validation loss is: 1.4924075603485107\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 145.0601978302002 \n",
            "\n",
            "Epoch 147 finished.\n",
            "Training loss is: 1.472208363342285\n",
            "Training accuracy is: 0.9897166666666667\n",
            "Validation loss is: 1.4924535751342773\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 145.0624828338623 \n",
            "\n",
            "Epoch 148 finished.\n",
            "Training loss is: 1.4721962944030762\n",
            "Training accuracy is: 0.9897333333333334\n",
            "Validation loss is: 1.4924148321151733\n",
            "Validation accuracy is: 0.9687\n",
            "L2 norm is 145.06476402282715 \n",
            "\n",
            "Epoch 149 finished.\n",
            "Training loss is: 1.4721808046976725\n",
            "Training accuracy is: 0.9897666666666667\n",
            "Validation loss is: 1.4923739433288574\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 145.06714868545532 \n",
            "\n",
            "Epoch 150 finished.\n",
            "Training loss is: 1.4721663495381674\n",
            "Training accuracy is: 0.9897666666666667\n",
            "Validation loss is: 1.4923609495162964\n",
            "Validation accuracy is: 0.9685\n",
            "L2 norm is 145.06957244873047 \n",
            "\n",
            "Epoch 151 finished.\n",
            "Training loss is: 1.4721539810180664\n",
            "Training accuracy is: 0.9897833333333333\n",
            "Validation loss is: 1.49229896068573\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 145.07189321517944 \n",
            "\n",
            "Epoch 152 finished.\n",
            "Training loss is: 1.4721506342569988\n",
            "Training accuracy is: 0.9897833333333333\n",
            "Validation loss is: 1.4923015832901\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 145.07428455352783 \n",
            "\n",
            "Epoch 153 finished.\n",
            "Training loss is: 1.4721349260965984\n",
            "Training accuracy is: 0.9898166666666667\n",
            "Validation loss is: 1.4922535419464111\n",
            "Validation accuracy is: 0.9689\n",
            "L2 norm is 145.07652807235718 \n",
            "\n",
            "Epoch 154 finished.\n",
            "Training loss is: 1.4722601186116537\n",
            "Training accuracy is: 0.9898333333333333\n",
            "Validation loss is: 1.492440104484558\n",
            "Validation accuracy is: 0.9681\n",
            "L2 norm is 145.07939386367798 \n",
            "\n",
            "Epoch 155 finished.\n",
            "Training loss is: 1.472096797688802\n",
            "Training accuracy is: 0.98985\n",
            "Validation loss is: 1.492370843887329\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.08182048797607 \n",
            "\n",
            "Epoch 156 finished.\n",
            "Training loss is: 1.4720560839335124\n",
            "Training accuracy is: 0.98985\n",
            "Validation loss is: 1.4923725128173828\n",
            "Validation accuracy is: 0.9687\n",
            "L2 norm is 145.084002494812 \n",
            "\n",
            "Epoch 157 finished.\n",
            "Training loss is: 1.472049446105957\n",
            "Training accuracy is: 0.9898666666666667\n",
            "Validation loss is: 1.492304801940918\n",
            "Validation accuracy is: 0.9694\n",
            "L2 norm is 145.08611679077148 \n",
            "\n",
            "Epoch 158 finished.\n",
            "Training loss is: 1.4720430452982585\n",
            "Training accuracy is: 0.9898666666666667\n",
            "Validation loss is: 1.4921956062316895\n",
            "Validation accuracy is: 0.9694\n",
            "L2 norm is 145.08821487426758 \n",
            "\n",
            "Epoch 159 finished.\n",
            "Training loss is: 1.4720535344441732\n",
            "Training accuracy is: 0.9898833333333333\n",
            "Validation loss is: 1.4923679828643799\n",
            "Validation accuracy is: 0.9688\n",
            "L2 norm is 145.09041595458984 \n",
            "\n",
            "Epoch 160 finished.\n",
            "Training loss is: 1.4720141400655111\n",
            "Training accuracy is: 0.9899\n",
            "Validation loss is: 1.49225914478302\n",
            "Validation accuracy is: 0.9693\n",
            "L2 norm is 145.092435836792 \n",
            "\n",
            "Epoch 161 finished.\n",
            "Training loss is: 1.4720148793538412\n",
            "Training accuracy is: 0.9899\n",
            "Validation loss is: 1.4922088384628296\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 145.09460544586182 \n",
            "\n",
            "Epoch 162 finished.\n",
            "Training loss is: 1.471997120666504\n",
            "Training accuracy is: 0.9899\n",
            "Validation loss is: 1.4921574592590332\n",
            "Validation accuracy is: 0.9693\n",
            "L2 norm is 145.0965781211853 \n",
            "\n",
            "Epoch 163 finished.\n",
            "Training loss is: 1.471997149149577\n",
            "Training accuracy is: 0.9899\n",
            "Validation loss is: 1.4921892881393433\n",
            "Validation accuracy is: 0.9694\n",
            "L2 norm is 145.09846353530884 \n",
            "\n",
            "Epoch 164 finished.\n",
            "Training loss is: 1.472039347076416\n",
            "Training accuracy is: 0.9899166666666667\n",
            "Validation loss is: 1.4921764135360718\n",
            "Validation accuracy is: 0.9689\n",
            "L2 norm is 145.10066080093384 \n",
            "\n",
            "Epoch 165 finished.\n",
            "Training loss is: 1.4719810768127441\n",
            "Training accuracy is: 0.9899333333333333\n",
            "Validation loss is: 1.492133617401123\n",
            "Validation accuracy is: 0.969\n",
            "L2 norm is 145.10263395309448 \n",
            "\n",
            "Epoch 166 finished.\n",
            "Training loss is: 1.4720377482096354\n",
            "Training accuracy is: 0.98995\n",
            "Validation loss is: 1.4920647144317627\n",
            "Validation accuracy is: 0.9694\n",
            "L2 norm is 145.10500717163086 \n",
            "\n",
            "Epoch 167 finished.\n",
            "Training loss is: 1.4720059481302896\n",
            "Training accuracy is: 0.9899666666666667\n",
            "Validation loss is: 1.4921810626983643\n",
            "Validation accuracy is: 0.9693\n",
            "L2 norm is 145.10727834701538 \n",
            "\n",
            "Epoch 168 finished.\n",
            "Training loss is: 1.47196466217041\n",
            "Training accuracy is: 0.99\n",
            "Validation loss is: 1.492107629776001\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.1094741821289 \n",
            "\n",
            "Epoch 169 finished.\n",
            "Training loss is: 1.4718965649922688\n",
            "Training accuracy is: 0.99\n",
            "Validation loss is: 1.4921085834503174\n",
            "Validation accuracy is: 0.9691\n",
            "L2 norm is 145.1114959716797 \n",
            "\n",
            "Epoch 170 finished.\n",
            "Training loss is: 1.4718934895833333\n",
            "Training accuracy is: 0.9900166666666667\n",
            "Validation loss is: 1.4921163320541382\n",
            "Validation accuracy is: 0.9689\n",
            "L2 norm is 145.11339950561523 \n",
            "\n",
            "Epoch 171 finished.\n",
            "Training loss is: 1.471886614481608\n",
            "Training accuracy is: 0.9900166666666667\n",
            "Validation loss is: 1.491984486579895\n",
            "Validation accuracy is: 0.9694\n",
            "L2 norm is 145.11537265777588 \n",
            "\n",
            "Epoch 172 finished.\n",
            "Training loss is: 1.471884169769287\n",
            "Training accuracy is: 0.9900333333333333\n",
            "Validation loss is: 1.492048978805542\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.11724376678467 \n",
            "\n",
            "Epoch 173 finished.\n",
            "Training loss is: 1.47186371383667\n",
            "Training accuracy is: 0.99005\n",
            "Validation loss is: 1.4919681549072266\n",
            "Validation accuracy is: 0.9696\n",
            "L2 norm is 145.1191577911377 \n",
            "\n",
            "Epoch 174 finished.\n",
            "Training loss is: 1.4718488558451335\n",
            "Training accuracy is: 0.99005\n",
            "Validation loss is: 1.4920607805252075\n",
            "Validation accuracy is: 0.9697\n",
            "L2 norm is 145.12106561660767 \n",
            "\n",
            "Epoch 175 finished.\n",
            "Training loss is: 1.4718485761006672\n",
            "Training accuracy is: 0.9900833333333333\n",
            "Validation loss is: 1.492017149925232\n",
            "Validation accuracy is: 0.9696\n",
            "L2 norm is 145.12304258346558 \n",
            "\n",
            "Epoch 176 finished.\n",
            "Training loss is: 1.4718107457478842\n",
            "Training accuracy is: 0.9900833333333333\n",
            "Validation loss is: 1.4919729232788086\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.12488460540771 \n",
            "\n",
            "Epoch 177 finished.\n",
            "Training loss is: 1.4718054982503255\n",
            "Training accuracy is: 0.9900833333333333\n",
            "Validation loss is: 1.492013931274414\n",
            "Validation accuracy is: 0.9696\n",
            "L2 norm is 145.1266803741455 \n",
            "\n",
            "Epoch 178 finished.\n",
            "Training loss is: 1.4717954686482748\n",
            "Training accuracy is: 0.9900833333333333\n",
            "Validation loss is: 1.4920077323913574\n",
            "Validation accuracy is: 0.9696\n",
            "L2 norm is 145.12841081619263 \n",
            "\n",
            "Epoch 179 finished.\n",
            "Training loss is: 1.4717993019104003\n",
            "Training accuracy is: 0.9900833333333333\n",
            "Validation loss is: 1.4920481443405151\n",
            "Validation accuracy is: 0.9697\n",
            "L2 norm is 145.13012504577637 \n",
            "\n",
            "Epoch 180 finished.\n",
            "Training loss is: 1.4717956314086913\n",
            "Training accuracy is: 0.9900833333333333\n",
            "Validation loss is: 1.492002010345459\n",
            "Validation accuracy is: 0.9697\n",
            "L2 norm is 145.1317834854126 \n",
            "\n",
            "Epoch 181 finished.\n",
            "Training loss is: 1.471799299875895\n",
            "Training accuracy is: 0.9900833333333333\n",
            "Validation loss is: 1.4920012950897217\n",
            "Validation accuracy is: 0.9698\n",
            "L2 norm is 145.13339471817017 \n",
            "\n",
            "Epoch 182 finished.\n",
            "Training loss is: 1.471796709696452\n",
            "Training accuracy is: 0.9901\n",
            "Validation loss is: 1.4919639825820923\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.1350975036621 \n",
            "\n",
            "Epoch 183 finished.\n",
            "Training loss is: 1.471784785715739\n",
            "Training accuracy is: 0.9901\n",
            "Validation loss is: 1.4920634031295776\n",
            "Validation accuracy is: 0.9695\n",
            "L2 norm is 145.1367163658142 \n",
            "\n",
            "Epoch 184 finished.\n",
            "Training loss is: 1.4717688873291015\n",
            "Training accuracy is: 0.9901\n",
            "Validation loss is: 1.4920095205307007\n",
            "Validation accuracy is: 0.9699\n",
            "L2 norm is 145.1383056640625 \n",
            "\n",
            "Epoch 185 finished.\n",
            "Training loss is: 1.47178878809611\n",
            "Training accuracy is: 0.9901166666666666\n",
            "Validation loss is: 1.4920878410339355\n",
            "Validation accuracy is: 0.9694\n",
            "L2 norm is 145.13988590240479 \n",
            "\n",
            "Epoch 186 finished.\n",
            "Training loss is: 1.4717736747741699\n",
            "Training accuracy is: 0.9901333333333333\n",
            "Validation loss is: 1.4920130968093872\n",
            "Validation accuracy is: 0.9695\n",
            "L2 norm is 145.14148998260498 \n",
            "\n",
            "Epoch 187 finished.\n",
            "Training loss is: 1.4717678642272949\n",
            "Training accuracy is: 0.9901333333333333\n",
            "Validation loss is: 1.492073893547058\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.14319276809692 \n",
            "\n",
            "Epoch 188 finished.\n",
            "Training loss is: 1.4717480651855468\n",
            "Training accuracy is: 0.9901333333333333\n",
            "Validation loss is: 1.4919954538345337\n",
            "Validation accuracy is: 0.9697\n",
            "L2 norm is 145.14475297927856 \n",
            "\n",
            "Epoch 189 finished.\n",
            "Training loss is: 1.4717454043070475\n",
            "Training accuracy is: 0.9901333333333333\n",
            "Validation loss is: 1.4920669794082642\n",
            "Validation accuracy is: 0.9693\n",
            "L2 norm is 145.1462688446045 \n",
            "\n",
            "Epoch 190 finished.\n",
            "Training loss is: 1.471737995147705\n",
            "Training accuracy is: 0.9901333333333333\n",
            "Validation loss is: 1.491990089416504\n",
            "Validation accuracy is: 0.9697\n",
            "L2 norm is 145.1477723121643 \n",
            "\n",
            "Epoch 191 finished.\n",
            "Training loss is: 1.4717317530314127\n",
            "Training accuracy is: 0.99015\n",
            "Validation loss is: 1.4919871091842651\n",
            "Validation accuracy is: 0.9694\n",
            "L2 norm is 145.149254322052 \n",
            "\n",
            "Epoch 192 finished.\n",
            "Training loss is: 1.4717434242248535\n",
            "Training accuracy is: 0.99015\n",
            "Validation loss is: 1.4919960498809814\n",
            "Validation accuracy is: 0.9698\n",
            "L2 norm is 145.1507544517517 \n",
            "\n",
            "Epoch 193 finished.\n",
            "Training loss is: 1.4717162045796712\n",
            "Training accuracy is: 0.99015\n",
            "Validation loss is: 1.4920084476470947\n",
            "Validation accuracy is: 0.9693\n",
            "L2 norm is 145.15223455429077 \n",
            "\n",
            "Epoch 194 finished.\n",
            "Training loss is: 1.4717139282226563\n",
            "Training accuracy is: 0.99015\n",
            "Validation loss is: 1.4919945001602173\n",
            "Validation accuracy is: 0.9691\n",
            "L2 norm is 145.1536741256714 \n",
            "\n",
            "Epoch 195 finished.\n",
            "Training loss is: 1.4717228108723959\n",
            "Training accuracy is: 0.99015\n",
            "Validation loss is: 1.491987943649292\n",
            "Validation accuracy is: 0.9693\n",
            "L2 norm is 145.15508222579956 \n",
            "\n",
            "Epoch 196 finished.\n",
            "Training loss is: 1.4717225102742513\n",
            "Training accuracy is: 0.99015\n",
            "Validation loss is: 1.4919058084487915\n",
            "Validation accuracy is: 0.9693\n",
            "L2 norm is 145.15646934509277 \n",
            "\n",
            "Epoch 197 finished.\n",
            "Training loss is: 1.4717212259928385\n",
            "Training accuracy is: 0.99015\n",
            "Validation loss is: 1.4919183254241943\n",
            "Validation accuracy is: 0.9696\n",
            "L2 norm is 145.15781259536743 \n",
            "\n",
            "Epoch 198 finished.\n",
            "Training loss is: 1.4717087966918945\n",
            "Training accuracy is: 0.99015\n",
            "Validation loss is: 1.4919158220291138\n",
            "Validation accuracy is: 0.9692\n",
            "L2 norm is 145.15915393829346 \n",
            "\n",
            "Epoch 199 finished.\n",
            "Training loss is: 1.4717238065083822\n",
            "Training accuracy is: 0.99015\n",
            "Validation loss is: 1.4919034242630005\n",
            "Validation accuracy is: 0.9698\n",
            "L2 norm is 145.1604495048523 \n",
            "\n",
            "Epoch 200 finished.\n",
            "Training loss is: 1.4717101267496744\n",
            "Training accuracy is: 0.9901666666666666\n",
            "Validation loss is: 1.491810917854309\n",
            "Validation accuracy is: 0.97\n",
            "L2 norm is 145.1618151664734 \n",
            "\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "_ = torch.manual_seed(0)\n",
        "mlp = MLP()\n",
        "mlp.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(mlp.parameters(), lr=.02, weight_decay=0)\n",
        "\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.type(torch.LongTensor).to(device)\n",
        "\n",
        "training_accs = []\n",
        "validation_accs = []\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(200):  # loop over the dataset multiple times\n",
        "\n",
        "    training_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss = loss.item() * batch_size\n",
        "        training_loss += batch_loss\n",
        "    \n",
        "    print(\"Epoch\", epoch+1, \"finished.\")\n",
        "\n",
        "    training_loss = training_loss/60000\n",
        "    training_losses.append(training_loss)\n",
        "    print(\"Training loss is:\", training_loss)\n",
        "\n",
        "    y_train_pred = mlp(x_train)\n",
        "    train_target = torch.argmax(y_train_pred, dim=1)\n",
        "    train_acc = torch.sum(train_target==y_train).item()/len(train_target)\n",
        "    training_accs.append(train_acc)\n",
        "    print(\"Training accuracy is:\", train_acc)\n",
        "    \n",
        "    y_pred = mlp(x_test).to(device)\n",
        "\n",
        "    loss = criterion(y_pred, y_test)\n",
        "    valid_loss = loss.item()\n",
        "    validation_losses.append(valid_loss)\n",
        "    print(\"Validation loss is:\", valid_loss)\n",
        "\n",
        "\n",
        "    target = torch.argmax(y_pred, dim=1)\n",
        "    acc = torch.sum(target==y_test)\n",
        "    validation_accs.append(acc)\n",
        "    print(\"Validation accuracy is:\", torch.sum(target==y_test).item()/(len(target)))\n",
        "\n",
        "    l2_norm = 0\n",
        "    for layer in mlp.model_logits:\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            t = layer.weight\n",
        "            l2_norm += torch.linalg.matrix_norm(t).item()\n",
        "    print(\"L2 norm is\", l2_norm, \"\\n\")\n",
        "\n",
        "    # print(f'Epoch {epoch+1} \\n Training Loss: {running_loss} \\n Validation Loss: {valid_loss / len(testloader)}')\n",
        "    # if epoch%20 == 0:\n",
        "    #     gcs = []\n",
        "    #     i = 0\n",
        "    #     while i < 10:\n",
        "    #         jacobian_logits = jacrev(mlp.model_logits)(x_train[i])\n",
        "    #         frob_square_norms = torch.square(torch.linalg.matrix_norm(jacobian_logits))\n",
        "    #         gcs.append(frob_square_norms.item())\n",
        "    #         i += 1\n",
        "    #     gc = sum(gcs)/len(gcs)\n",
        "    #     print(\"Approximate GC:\", gc)\n",
        "#     print(frob_square_norms)\n",
        "#     print(\"EPOCH\", epoch+1,\"GEOMETRIC COMPLEXITY:\")\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9aByoPXpI5X",
        "outputId": "0dd2e70f-6272-4cf8-c446-f5a406bdb14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/My Drive/')"
      ],
      "metadata": {
        "id": "F0FHeRVN4fNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JXK_K6iJ4w-L",
        "outputId": "57abf06f-583f-4262-bc2f-279ff5e5d0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "training_stats = [\n",
        "    training_accs,\n",
        "    validation_accs,\n",
        "    training_losses,\n",
        "    validation_losses\n",
        "]\n",
        "\n",
        "output_file = open('6HL_500W_LR02_MNIST_MODEL_DATA.bin', 'wb')\n",
        "pickle.dump(training_stats, output_file)\n",
        "output_file.close()"
      ],
      "metadata": {
        "id": "Nt01tN8CzjSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = pickle.load(open('6HL_500W_LR02_MNIST_MODEL_DATA.bin', 'rb'))\n",
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R5O3QE11Fit",
        "outputId": "7bf493d4-b495-419b-8e3d-1a06d32baef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.17813333333333334,\n",
              "  0.14108333333333334,\n",
              "  0.13623333333333335,\n",
              "  0.16415,\n",
              "  0.23051666666666668,\n",
              "  0.29636666666666667,\n",
              "  0.31211666666666665,\n",
              "  0.2058,\n",
              "  0.3596666666666667,\n",
              "  0.6227,\n",
              "  0.7253,\n",
              "  0.74265,\n",
              "  0.7515,\n",
              "  0.75555,\n",
              "  0.7624833333333333,\n",
              "  0.7634333333333333,\n",
              "  0.7655,\n",
              "  0.7710166666666667,\n",
              "  0.7732666666666667,\n",
              "  0.7759,\n",
              "  0.7750833333333333,\n",
              "  0.7791166666666667,\n",
              "  0.7799666666666667,\n",
              "  0.7819833333333334,\n",
              "  0.78325,\n",
              "  0.7840833333333334,\n",
              "  0.78375,\n",
              "  0.78675,\n",
              "  0.7874333333333333,\n",
              "  0.7853333333333333,\n",
              "  0.79025,\n",
              "  0.7902833333333333,\n",
              "  0.7915166666666666,\n",
              "  0.7928,\n",
              "  0.79275,\n",
              "  0.7934666666666667,\n",
              "  0.7935,\n",
              "  0.7948333333333333,\n",
              "  0.7959833333333334,\n",
              "  0.8602,\n",
              "  0.8762166666666666,\n",
              "  0.9429666666666666,\n",
              "  0.9449666666666666,\n",
              "  0.9548166666666666,\n",
              "  0.9564333333333334,\n",
              "  0.9633166666666667,\n",
              "  0.96695,\n",
              "  0.97065,\n",
              "  0.9718,\n",
              "  0.9723666666666667,\n",
              "  0.97515,\n",
              "  0.97425,\n",
              "  0.97195,\n",
              "  0.9774333333333334,\n",
              "  0.97425,\n",
              "  0.9791666666666666,\n",
              "  0.9787333333333333,\n",
              "  0.9785,\n",
              "  0.9794833333333334,\n",
              "  0.9821333333333333,\n",
              "  0.9835333333333334,\n",
              "  0.9839833333333333,\n",
              "  0.9838666666666667,\n",
              "  0.9848166666666667,\n",
              "  0.9847833333333333,\n",
              "  0.9843833333333334,\n",
              "  0.9849166666666667,\n",
              "  0.9858166666666667,\n",
              "  0.98495,\n",
              "  0.9859333333333333,\n",
              "  0.9864,\n",
              "  0.9863833333333333,\n",
              "  0.9867,\n",
              "  0.98695,\n",
              "  0.9868666666666667,\n",
              "  0.9872833333333333,\n",
              "  0.98725,\n",
              "  0.98655,\n",
              "  0.98765,\n",
              "  0.98765,\n",
              "  0.9878833333333333,\n",
              "  0.9879666666666667,\n",
              "  0.9880833333333333,\n",
              "  0.9882,\n",
              "  0.9882166666666666,\n",
              "  0.98835,\n",
              "  0.9883666666666666,\n",
              "  0.9883666666666666,\n",
              "  0.98845,\n",
              "  0.9885166666666667,\n",
              "  0.98855,\n",
              "  0.9885833333333334,\n",
              "  0.9887333333333334,\n",
              "  0.9887333333333334,\n",
              "  0.9887666666666667,\n",
              "  0.9888,\n",
              "  0.98885,\n",
              "  0.9889,\n",
              "  0.9889333333333333,\n",
              "  0.9889833333333333,\n",
              "  0.9890166666666667,\n",
              "  0.98905,\n",
              "  0.98905,\n",
              "  0.9890833333333333,\n",
              "  0.9891166666666666,\n",
              "  0.98915,\n",
              "  0.9891666666666666,\n",
              "  0.9891666666666666,\n",
              "  0.9891666666666666,\n",
              "  0.9892,\n",
              "  0.9892166666666666,\n",
              "  0.9892333333333333,\n",
              "  0.9892333333333333,\n",
              "  0.9892666666666666,\n",
              "  0.9893,\n",
              "  0.9893,\n",
              "  0.9893166666666666,\n",
              "  0.9893333333333333,\n",
              "  0.9893333333333333,\n",
              "  0.98935,\n",
              "  0.9893833333333333,\n",
              "  0.9894166666666667,\n",
              "  0.9894166666666667,\n",
              "  0.9894166666666667,\n",
              "  0.9894333333333334,\n",
              "  0.9894666666666667,\n",
              "  0.9894666666666667,\n",
              "  0.9894833333333334,\n",
              "  0.9895,\n",
              "  0.9895166666666667,\n",
              "  0.9895333333333334,\n",
              "  0.9895333333333334,\n",
              "  0.9895333333333334,\n",
              "  0.98955,\n",
              "  0.98955,\n",
              "  0.98955,\n",
              "  0.98955,\n",
              "  0.98955,\n",
              "  0.9895666666666667,\n",
              "  0.9895833333333334,\n",
              "  0.9896166666666667,\n",
              "  0.9896333333333334,\n",
              "  0.9896666666666667,\n",
              "  0.9896666666666667,\n",
              "  0.9897,\n",
              "  0.9897,\n",
              "  0.9897166666666667,\n",
              "  0.9897333333333334,\n",
              "  0.9897666666666667,\n",
              "  0.9897666666666667,\n",
              "  0.9897833333333333,\n",
              "  0.9897833333333333,\n",
              "  0.9898166666666667,\n",
              "  0.9898333333333333,\n",
              "  0.98985,\n",
              "  0.98985,\n",
              "  0.9898666666666667,\n",
              "  0.9898666666666667,\n",
              "  0.9898833333333333,\n",
              "  0.9899,\n",
              "  0.9899,\n",
              "  0.9899,\n",
              "  0.9899,\n",
              "  0.9899166666666667,\n",
              "  0.9899333333333333,\n",
              "  0.98995,\n",
              "  0.9899666666666667,\n",
              "  0.99,\n",
              "  0.99,\n",
              "  0.9900166666666667,\n",
              "  0.9900166666666667,\n",
              "  0.9900333333333333,\n",
              "  0.99005,\n",
              "  0.99005,\n",
              "  0.9900833333333333,\n",
              "  0.9900833333333333,\n",
              "  0.9900833333333333,\n",
              "  0.9900833333333333,\n",
              "  0.9900833333333333,\n",
              "  0.9900833333333333,\n",
              "  0.9900833333333333,\n",
              "  0.9901,\n",
              "  0.9901,\n",
              "  0.9901,\n",
              "  0.9901166666666666,\n",
              "  0.9901333333333333,\n",
              "  0.9901333333333333,\n",
              "  0.9901333333333333,\n",
              "  0.9901333333333333,\n",
              "  0.9901333333333333,\n",
              "  0.99015,\n",
              "  0.99015,\n",
              "  0.99015,\n",
              "  0.99015,\n",
              "  0.99015,\n",
              "  0.99015,\n",
              "  0.99015,\n",
              "  0.99015,\n",
              "  0.99015,\n",
              "  0.9901666666666666],\n",
              " [tensor(1790, device='cuda:0'),\n",
              "  tensor(1444, device='cuda:0'),\n",
              "  tensor(1386, device='cuda:0'),\n",
              "  tensor(1664, device='cuda:0'),\n",
              "  tensor(2343, device='cuda:0'),\n",
              "  tensor(3025, device='cuda:0'),\n",
              "  tensor(3124, device='cuda:0'),\n",
              "  tensor(2069, device='cuda:0'),\n",
              "  tensor(3635, device='cuda:0'),\n",
              "  tensor(6272, device='cuda:0'),\n",
              "  tensor(7336, device='cuda:0'),\n",
              "  tensor(7479, device='cuda:0'),\n",
              "  tensor(7547, device='cuda:0'),\n",
              "  tensor(7601, device='cuda:0'),\n",
              "  tensor(7649, device='cuda:0'),\n",
              "  tensor(7649, device='cuda:0'),\n",
              "  tensor(7661, device='cuda:0'),\n",
              "  tensor(7717, device='cuda:0'),\n",
              "  tensor(7739, device='cuda:0'),\n",
              "  tensor(7759, device='cuda:0'),\n",
              "  tensor(7742, device='cuda:0'),\n",
              "  tensor(7780, device='cuda:0'),\n",
              "  tensor(7793, device='cuda:0'),\n",
              "  tensor(7796, device='cuda:0'),\n",
              "  tensor(7815, device='cuda:0'),\n",
              "  tensor(7810, device='cuda:0'),\n",
              "  tensor(7817, device='cuda:0'),\n",
              "  tensor(7843, device='cuda:0'),\n",
              "  tensor(7842, device='cuda:0'),\n",
              "  tensor(7813, device='cuda:0'),\n",
              "  tensor(7855, device='cuda:0'),\n",
              "  tensor(7861, device='cuda:0'),\n",
              "  tensor(7870, device='cuda:0'),\n",
              "  tensor(7875, device='cuda:0'),\n",
              "  tensor(7867, device='cuda:0'),\n",
              "  tensor(7876, device='cuda:0'),\n",
              "  tensor(7871, device='cuda:0'),\n",
              "  tensor(7884, device='cuda:0'),\n",
              "  tensor(7894, device='cuda:0'),\n",
              "  tensor(8546, device='cuda:0'),\n",
              "  tensor(8693, device='cuda:0'),\n",
              "  tensor(9358, device='cuda:0'),\n",
              "  tensor(9359, device='cuda:0'),\n",
              "  tensor(9466, device='cuda:0'),\n",
              "  tensor(9471, device='cuda:0'),\n",
              "  tensor(9524, device='cuda:0'),\n",
              "  tensor(9572, device='cuda:0'),\n",
              "  tensor(9563, device='cuda:0'),\n",
              "  tensor(9608, device='cuda:0'),\n",
              "  tensor(9593, device='cuda:0'),\n",
              "  tensor(9629, device='cuda:0'),\n",
              "  tensor(9617, device='cuda:0'),\n",
              "  tensor(9574, device='cuda:0'),\n",
              "  tensor(9631, device='cuda:0'),\n",
              "  tensor(9606, device='cuda:0'),\n",
              "  tensor(9625, device='cuda:0'),\n",
              "  tensor(9619, device='cuda:0'),\n",
              "  tensor(9608, device='cuda:0'),\n",
              "  tensor(9614, device='cuda:0'),\n",
              "  tensor(9652, device='cuda:0'),\n",
              "  tensor(9654, device='cuda:0'),\n",
              "  tensor(9652, device='cuda:0'),\n",
              "  tensor(9652, device='cuda:0'),\n",
              "  tensor(9672, device='cuda:0'),\n",
              "  tensor(9660, device='cuda:0'),\n",
              "  tensor(9648, device='cuda:0'),\n",
              "  tensor(9664, device='cuda:0'),\n",
              "  tensor(9673, device='cuda:0'),\n",
              "  tensor(9655, device='cuda:0'),\n",
              "  tensor(9669, device='cuda:0'),\n",
              "  tensor(9671, device='cuda:0'),\n",
              "  tensor(9667, device='cuda:0'),\n",
              "  tensor(9678, device='cuda:0'),\n",
              "  tensor(9671, device='cuda:0'),\n",
              "  tensor(9675, device='cuda:0'),\n",
              "  tensor(9677, device='cuda:0'),\n",
              "  tensor(9673, device='cuda:0'),\n",
              "  tensor(9661, device='cuda:0'),\n",
              "  tensor(9669, device='cuda:0'),\n",
              "  tensor(9673, device='cuda:0'),\n",
              "  tensor(9672, device='cuda:0'),\n",
              "  tensor(9681, device='cuda:0'),\n",
              "  tensor(9677, device='cuda:0'),\n",
              "  tensor(9675, device='cuda:0'),\n",
              "  tensor(9682, device='cuda:0'),\n",
              "  tensor(9678, device='cuda:0'),\n",
              "  tensor(9677, device='cuda:0'),\n",
              "  tensor(9685, device='cuda:0'),\n",
              "  tensor(9684, device='cuda:0'),\n",
              "  tensor(9677, device='cuda:0'),\n",
              "  tensor(9672, device='cuda:0'),\n",
              "  tensor(9669, device='cuda:0'),\n",
              "  tensor(9679, device='cuda:0'),\n",
              "  tensor(9678, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9684, device='cuda:0'),\n",
              "  tensor(9681, device='cuda:0'),\n",
              "  tensor(9683, device='cuda:0'),\n",
              "  tensor(9686, device='cuda:0'),\n",
              "  tensor(9681, device='cuda:0'),\n",
              "  tensor(9682, device='cuda:0'),\n",
              "  tensor(9680, device='cuda:0'),\n",
              "  tensor(9683, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9685, device='cuda:0'),\n",
              "  tensor(9687, device='cuda:0'),\n",
              "  tensor(9685, device='cuda:0'),\n",
              "  tensor(9684, device='cuda:0'),\n",
              "  tensor(9686, device='cuda:0'),\n",
              "  tensor(9685, device='cuda:0'),\n",
              "  tensor(9687, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9683, device='cuda:0'),\n",
              "  tensor(9687, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9683, device='cuda:0'),\n",
              "  tensor(9685, device='cuda:0'),\n",
              "  tensor(9682, device='cuda:0'),\n",
              "  tensor(9686, device='cuda:0'),\n",
              "  tensor(9691, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9689, device='cuda:0'),\n",
              "  tensor(9687, device='cuda:0'),\n",
              "  tensor(9686, device='cuda:0'),\n",
              "  tensor(9684, device='cuda:0'),\n",
              "  tensor(9687, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9686, device='cuda:0'),\n",
              "  tensor(9693, device='cuda:0'),\n",
              "  tensor(9693, device='cuda:0'),\n",
              "  tensor(9689, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9684, device='cuda:0'),\n",
              "  tensor(9689, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9687, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9685, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9689, device='cuda:0'),\n",
              "  tensor(9681, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9687, device='cuda:0'),\n",
              "  tensor(9694, device='cuda:0'),\n",
              "  tensor(9694, device='cuda:0'),\n",
              "  tensor(9688, device='cuda:0'),\n",
              "  tensor(9693, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9693, device='cuda:0'),\n",
              "  tensor(9694, device='cuda:0'),\n",
              "  tensor(9689, device='cuda:0'),\n",
              "  tensor(9690, device='cuda:0'),\n",
              "  tensor(9694, device='cuda:0'),\n",
              "  tensor(9693, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9691, device='cuda:0'),\n",
              "  tensor(9689, device='cuda:0'),\n",
              "  tensor(9694, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9696, device='cuda:0'),\n",
              "  tensor(9697, device='cuda:0'),\n",
              "  tensor(9696, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9696, device='cuda:0'),\n",
              "  tensor(9696, device='cuda:0'),\n",
              "  tensor(9697, device='cuda:0'),\n",
              "  tensor(9697, device='cuda:0'),\n",
              "  tensor(9698, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9695, device='cuda:0'),\n",
              "  tensor(9699, device='cuda:0'),\n",
              "  tensor(9694, device='cuda:0'),\n",
              "  tensor(9695, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9697, device='cuda:0'),\n",
              "  tensor(9693, device='cuda:0'),\n",
              "  tensor(9697, device='cuda:0'),\n",
              "  tensor(9694, device='cuda:0'),\n",
              "  tensor(9698, device='cuda:0'),\n",
              "  tensor(9693, device='cuda:0'),\n",
              "  tensor(9691, device='cuda:0'),\n",
              "  tensor(9693, device='cuda:0'),\n",
              "  tensor(9693, device='cuda:0'),\n",
              "  tensor(9696, device='cuda:0'),\n",
              "  tensor(9692, device='cuda:0'),\n",
              "  tensor(9698, device='cuda:0'),\n",
              "  tensor(9700, device='cuda:0')],\n",
              " [2.3037174743652344,\n",
              "  2.303323304748535,\n",
              "  2.3028902155558266,\n",
              "  2.302337832132975,\n",
              "  2.3015418340047202,\n",
              "  2.3002661849975587,\n",
              "  2.297815846761068,\n",
              "  2.289237989807129,\n",
              "  2.2228543431599936,\n",
              "  1.997040636698405,\n",
              "  1.797225200398763,\n",
              "  1.737117800394694,\n",
              "  1.720102081044515,\n",
              "  1.7110954750061036,\n",
              "  1.7054203893025717,\n",
              "  1.7007265419006348,\n",
              "  1.6975089421590168,\n",
              "  1.6942366775512696,\n",
              "  1.6920197240193684,\n",
              "  1.6896511049906413,\n",
              "  1.6873746037801107,\n",
              "  1.6856619336446126,\n",
              "  1.6840942392985025,\n",
              "  1.682423459370931,\n",
              "  1.680818379720052,\n",
              "  1.6793556027730305,\n",
              "  1.678283127339681,\n",
              "  1.6769110911051432,\n",
              "  1.6756246119181315,\n",
              "  1.6745480072021484,\n",
              "  1.6736382614135743,\n",
              "  1.6721701784769694,\n",
              "  1.6715120051066081,\n",
              "  1.6704887639363606,\n",
              "  1.6698134012858072,\n",
              "  1.6688835202534993,\n",
              "  1.6678558184305827,\n",
              "  1.6669055890401205,\n",
              "  1.6662007016499838,\n",
              "  1.6010369756062826,\n",
              "  1.5900628954569498,\n",
              "  1.551828224690755,\n",
              "  1.52493169631958,\n",
              "  1.5158614382425943,\n",
              "  1.5100186681111654,\n",
              "  1.506621319325765,\n",
              "  1.502819925181071,\n",
              "  1.4998047154744467,\n",
              "  1.4973587567647297,\n",
              "  1.4952315002441405,\n",
              "  1.4932379880269369,\n",
              "  1.49132572555542,\n",
              "  1.4901013982137044,\n",
              "  1.4887199073791504,\n",
              "  1.4872596972147625,\n",
              "  1.486040042368571,\n",
              "  1.4855476048787435,\n",
              "  1.484342713165283,\n",
              "  1.48357090326945,\n",
              "  1.4824206301371257,\n",
              "  1.482004508972168,\n",
              "  1.48134593073527,\n",
              "  1.4810725410461425,\n",
              "  1.4799501736958822,\n",
              "  1.4795633786519369,\n",
              "  1.4790485186258953,\n",
              "  1.4785119669596354,\n",
              "  1.4780567159016926,\n",
              "  1.477569300587972,\n",
              "  1.4772361956278484,\n",
              "  1.4770248140970865,\n",
              "  1.476854071553548,\n",
              "  1.4765535001118977,\n",
              "  1.476053687286377,\n",
              "  1.4758582954406738,\n",
              "  1.475745427195231,\n",
              "  1.4756417922973633,\n",
              "  1.4753889246622722,\n",
              "  1.4749934511820475,\n",
              "  1.4748400891621907,\n",
              "  1.4747826021830241,\n",
              "  1.474565608215332,\n",
              "  1.4745397300720215,\n",
              "  1.474359964243571,\n",
              "  1.4742014556884766,\n",
              "  1.474139826965332,\n",
              "  1.4741025652567545,\n",
              "  1.473822511291504,\n",
              "  1.473746260325114,\n",
              "  1.4738321184794108,\n",
              "  1.4736644104003906,\n",
              "  1.4736365414937338,\n",
              "  1.4735724090576172,\n",
              "  1.4734152135213217,\n",
              "  1.473386720530192,\n",
              "  1.4733226483662922,\n",
              "  1.4733709645589192,\n",
              "  1.4732480410257975,\n",
              "  1.4731856765747071,\n",
              "  1.4731331797281901,\n",
              "  1.4731235743204754,\n",
              "  1.4731231218973795,\n",
              "  1.473111671447754,\n",
              "  1.4729799395243326,\n",
              "  1.4733100682576497,\n",
              "  1.4729490402221679,\n",
              "  1.4728792078653972,\n",
              "  1.4728650825500489,\n",
              "  1.472819297281901,\n",
              "  1.4728030934651692,\n",
              "  1.4728279912312825,\n",
              "  1.4727833541870117,\n",
              "  1.4727537963867188,\n",
              "  1.4727556800842285,\n",
              "  1.472720947519938,\n",
              "  1.4726761622111002,\n",
              "  1.4726678479512532,\n",
              "  1.4726620971679687,\n",
              "  1.47262199122111,\n",
              "  1.472606411997477,\n",
              "  1.4726085978190104,\n",
              "  1.4725941869099934,\n",
              "  1.4725492403666178,\n",
              "  1.4725292681376139,\n",
              "  1.4725281776428223,\n",
              "  1.472611267344157,\n",
              "  1.4724967280069987,\n",
              "  1.4724586329142253,\n",
              "  1.4724550819396973,\n",
              "  1.4724356803894043,\n",
              "  1.4724137079874675,\n",
              "  1.4723940205891928,\n",
              "  1.472381672668457,\n",
              "  1.4723832293192545,\n",
              "  1.4723711168924967,\n",
              "  1.4723617223103842,\n",
              "  1.4723577087402344,\n",
              "  1.4723487981160481,\n",
              "  1.4723575378417968,\n",
              "  1.4723390963236491,\n",
              "  1.4723419525146484,\n",
              "  1.4723213656107585,\n",
              "  1.4723265904744467,\n",
              "  1.4722499972025553,\n",
              "  1.4722384086608886,\n",
              "  1.4722262217203776,\n",
              "  1.472208363342285,\n",
              "  1.4721962944030762,\n",
              "  1.4721808046976725,\n",
              "  1.4721663495381674,\n",
              "  1.4721539810180664,\n",
              "  1.4721506342569988,\n",
              "  1.4721349260965984,\n",
              "  1.4722601186116537,\n",
              "  1.472096797688802,\n",
              "  1.4720560839335124,\n",
              "  1.472049446105957,\n",
              "  1.4720430452982585,\n",
              "  1.4720535344441732,\n",
              "  1.4720141400655111,\n",
              "  1.4720148793538412,\n",
              "  1.471997120666504,\n",
              "  1.471997149149577,\n",
              "  1.472039347076416,\n",
              "  1.4719810768127441,\n",
              "  1.4720377482096354,\n",
              "  1.4720059481302896,\n",
              "  1.47196466217041,\n",
              "  1.4718965649922688,\n",
              "  1.4718934895833333,\n",
              "  1.471886614481608,\n",
              "  1.471884169769287,\n",
              "  1.47186371383667,\n",
              "  1.4718488558451335,\n",
              "  1.4718485761006672,\n",
              "  1.4718107457478842,\n",
              "  1.4718054982503255,\n",
              "  1.4717954686482748,\n",
              "  1.4717993019104003,\n",
              "  1.4717956314086913,\n",
              "  1.471799299875895,\n",
              "  1.471796709696452,\n",
              "  1.471784785715739,\n",
              "  1.4717688873291015,\n",
              "  1.47178878809611,\n",
              "  1.4717736747741699,\n",
              "  1.4717678642272949,\n",
              "  1.4717480651855468,\n",
              "  1.4717454043070475,\n",
              "  1.471737995147705,\n",
              "  1.4717317530314127,\n",
              "  1.4717434242248535,\n",
              "  1.4717162045796712,\n",
              "  1.4717139282226563,\n",
              "  1.4717228108723959,\n",
              "  1.4717225102742513,\n",
              "  1.4717212259928385,\n",
              "  1.4717087966918945,\n",
              "  1.4717238065083822,\n",
              "  1.4717101267496744],\n",
              " [2.302281141281128,\n",
              "  2.3018643856048584,\n",
              "  2.3013722896575928,\n",
              "  2.300708770751953,\n",
              "  2.299710273742676,\n",
              "  2.2980058193206787,\n",
              "  2.294332265853882,\n",
              "  2.2696425914764404,\n",
              "  2.15262508392334,\n",
              "  1.861185908317566,\n",
              "  1.7422152757644653,\n",
              "  1.7192031145095825,\n",
              "  1.7087314128875732,\n",
              "  1.7049109935760498,\n",
              "  1.698136329650879,\n",
              "  1.6973549127578735,\n",
              "  1.6959549188613892,\n",
              "  1.690798044204712,\n",
              "  1.6881009340286255,\n",
              "  1.6858546733856201,\n",
              "  1.6869416236877441,\n",
              "  1.6829155683517456,\n",
              "  1.6823962926864624,\n",
              "  1.6810168027877808,\n",
              "  1.6793873310089111,\n",
              "  1.6790423393249512,\n",
              "  1.6794993877410889,\n",
              "  1.6767476797103882,\n",
              "  1.676313042640686,\n",
              "  1.6783783435821533,\n",
              "  1.6745017766952515,\n",
              "  1.6741576194763184,\n",
              "  1.6731278896331787,\n",
              "  1.6725980043411255,\n",
              "  1.6730144023895264,\n",
              "  1.6722660064697266,\n",
              "  1.6722371578216553,\n",
              "  1.6710072755813599,\n",
              "  1.6681898832321167,\n",
              "  1.6115756034851074,\n",
              "  1.5933663845062256,\n",
              "  1.530996561050415,\n",
              "  1.5304607152938843,\n",
              "  1.517759084701538,\n",
              "  1.5165419578552246,\n",
              "  1.511242389678955,\n",
              "  1.506910800933838,\n",
              "  1.5051333904266357,\n",
              "  1.5032652616500854,\n",
              "  1.5033683776855469,\n",
              "  1.5005193948745728,\n",
              "  1.5010658502578735,\n",
              "  1.504889726638794,\n",
              "  1.4997501373291016,\n",
              "  1.502687931060791,\n",
              "  1.4997323751449585,\n",
              "  1.4997339248657227,\n",
              "  1.5017921924591064,\n",
              "  1.5007396936416626,\n",
              "  1.4971915483474731,\n",
              "  1.4964832067489624,\n",
              "  1.4961012601852417,\n",
              "  1.496060848236084,\n",
              "  1.4951456785202026,\n",
              "  1.4956318140029907,\n",
              "  1.496677279472351,\n",
              "  1.495981216430664,\n",
              "  1.4946733713150024,\n",
              "  1.4961274862289429,\n",
              "  1.4950389862060547,\n",
              "  1.494912028312683,\n",
              "  1.4949569702148438,\n",
              "  1.494131088256836,\n",
              "  1.4938745498657227,\n",
              "  1.4941611289978027,\n",
              "  1.494143009185791,\n",
              "  1.494001865386963,\n",
              "  1.495300054550171,\n",
              "  1.4939500093460083,\n",
              "  1.494271993637085,\n",
              "  1.4940253496170044,\n",
              "  1.4930930137634277,\n",
              "  1.4934000968933105,\n",
              "  1.4935424327850342,\n",
              "  1.4934489727020264,\n",
              "  1.4933329820632935,\n",
              "  1.4934121370315552,\n",
              "  1.4930005073547363,\n",
              "  1.4930119514465332,\n",
              "  1.4930661916732788,\n",
              "  1.4933336973190308,\n",
              "  1.493944525718689,\n",
              "  1.4929804801940918,\n",
              "  1.4928407669067383,\n",
              "  1.4926506280899048,\n",
              "  1.492856502532959,\n",
              "  1.4930404424667358,\n",
              "  1.4928597211837769,\n",
              "  1.4925756454467773,\n",
              "  1.4928022623062134,\n",
              "  1.4925661087036133,\n",
              "  1.4926886558532715,\n",
              "  1.4928480386734009,\n",
              "  1.4926559925079346,\n",
              "  1.492630124092102,\n",
              "  1.4927823543548584,\n",
              "  1.492477536201477,\n",
              "  1.4923955202102661,\n",
              "  1.4924474954605103,\n",
              "  1.4924265146255493,\n",
              "  1.4925516843795776,\n",
              "  1.4924559593200684,\n",
              "  1.4923099279403687,\n",
              "  1.4924516677856445,\n",
              "  1.4928548336029053,\n",
              "  1.4924452304840088,\n",
              "  1.4924871921539307,\n",
              "  1.4926241636276245,\n",
              "  1.4925847053527832,\n",
              "  1.4923888444900513,\n",
              "  1.4924331903457642,\n",
              "  1.4924637079238892,\n",
              "  1.4923902750015259,\n",
              "  1.4925754070281982,\n",
              "  1.4924405813217163,\n",
              "  1.4924967288970947,\n",
              "  1.4925150871276855,\n",
              "  1.4925775527954102,\n",
              "  1.4927663803100586,\n",
              "  1.49247407913208,\n",
              "  1.4925298690795898,\n",
              "  1.4924269914627075,\n",
              "  1.4924795627593994,\n",
              "  1.4924161434173584,\n",
              "  1.4924201965332031,\n",
              "  1.4923973083496094,\n",
              "  1.4925192594528198,\n",
              "  1.492458701133728,\n",
              "  1.4924417734146118,\n",
              "  1.492531418800354,\n",
              "  1.4925568103790283,\n",
              "  1.4923005104064941,\n",
              "  1.4926458597183228,\n",
              "  1.4926230907440186,\n",
              "  1.4923189878463745,\n",
              "  1.4924075603485107,\n",
              "  1.4924535751342773,\n",
              "  1.4924148321151733,\n",
              "  1.4923739433288574,\n",
              "  1.4923609495162964,\n",
              "  1.49229896068573,\n",
              "  1.4923015832901,\n",
              "  1.4922535419464111,\n",
              "  1.492440104484558,\n",
              "  1.492370843887329,\n",
              "  1.4923725128173828,\n",
              "  1.492304801940918,\n",
              "  1.4921956062316895,\n",
              "  1.4923679828643799,\n",
              "  1.49225914478302,\n",
              "  1.4922088384628296,\n",
              "  1.4921574592590332,\n",
              "  1.4921892881393433,\n",
              "  1.4921764135360718,\n",
              "  1.492133617401123,\n",
              "  1.4920647144317627,\n",
              "  1.4921810626983643,\n",
              "  1.492107629776001,\n",
              "  1.4921085834503174,\n",
              "  1.4921163320541382,\n",
              "  1.491984486579895,\n",
              "  1.492048978805542,\n",
              "  1.4919681549072266,\n",
              "  1.4920607805252075,\n",
              "  1.492017149925232,\n",
              "  1.4919729232788086,\n",
              "  1.492013931274414,\n",
              "  1.4920077323913574,\n",
              "  1.4920481443405151,\n",
              "  1.492002010345459,\n",
              "  1.4920012950897217,\n",
              "  1.4919639825820923,\n",
              "  1.4920634031295776,\n",
              "  1.4920095205307007,\n",
              "  1.4920878410339355,\n",
              "  1.4920130968093872,\n",
              "  1.492073893547058,\n",
              "  1.4919954538345337,\n",
              "  1.4920669794082642,\n",
              "  1.491990089416504,\n",
              "  1.4919871091842651,\n",
              "  1.4919960498809814,\n",
              "  1.4920084476470947,\n",
              "  1.4919945001602173,\n",
              "  1.491987943649292,\n",
              "  1.4919058084487915,\n",
              "  1.4919183254241943,\n",
              "  1.4919158220291138,\n",
              "  1.4919034242630005,\n",
              "  1.491810917854309]]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KD5bJ2I02Orn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}